---
title: "MachineLearning_Assignment 6"
output: word_document
date: "2023-02-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE,echo = TRUE)
```

Loading the necessary libraries:
```{r load libraries}
library(tidyverse) 
library(readr)
library(caret)
library(klaR)
library(rpart)
library(rpart.plot)
library(pROC)
library(NHANES)
options(tibble.print_min = 5)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Q1 Loading NHANES data and limiting to 11 required variables; partitioning the data to 70/30:
```{r}
set.seed(123)
data(NHANES)
  
nhanes_hw6 = NHANES[c("Age", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100")] %>%
 na.omit()

summary(nhanes_hw6)
summary(nhanes_hw6$Diabetes)

diab_split = createDataPartition(nhanes_hw6$Diabetes, p = 0.7, list = FALSE)
train = nhanes_hw6[diab_split, ]
test = nhanes_hw6[-diab_split, ]
```
#### The outcome Diabetes is unbalanced.

## Q2 Creating prediction models
### A. Classification tree
```{r}
set.seed(123)
#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
ctrl = trainControl(method = "cv", number = 10, sampling = "down")

#Create sequence of cp parameters
grid.2 = expand.grid(cp=seq(0.001, 0.3, by=0.01))

#Train model
class_tree = train(Diabetes ~ ., data = train, method = "rpart", trControl = ctrl, tuneGrid=grid.2)

class_tree$bestTune
class_tree

rpart.plot(class_tree$finalModel)

#Important variable
varImp(class_tree)
```

### B. Support vector classifier
```{r}

set.seed(123)
#Creating 10-fold cross-validation 
cv =trainControl(method="cv", number=10, sampling = "down")

#Train model
svm_mod = train(Diabetes ~ ., data = train, method = "svmLinear", trControl = cv, preProcess=c("center", "scale"), tuneGrid=expand.grid(C=seq(0.001,2, length=30)))

#Visualizing accuracy versus values of C
plot(svm_mod)

#information about final model
svm_mod$finalModel
```

### C. logistic regression
```{r}
set.seed(123)
cv = trainControl(method = "cv", number = 10, sampling = "down")

lr_mod = train(
  Diabetes ~ ., 
  data = train, 
  method = "glm", 
  trControl = cv, 
  family = "binomial",
  preProcess=c("center", "scale"),
  tuneLength = 10
)
```

## Q3 comparing the models in the training data
```{r}
confusionMatrix(class_tree)
confusionMatrix(svm_mod)
lr_mod$results
```
#### The model with highest accuracy is the optimal predictor model. Logistic regression algorithm will be used as the optimal predictor model because it has the highest accuracy among the 3 models (0.7148). The Suuport vector classifier algorithm model has the accuracy of 0.7126, and classification tree algorithm accuracy is 0.7045. 

## Q4 Applying optimal prediction model in test data
```{r}
set.seed(123)
lr_pred = predict(lr_mod, test)
lr_cm = confusionMatrix(data= lr_pred, reference=test$Diabetes, positive="Yes")

lr_cm
```
#### On applying the logistic regression model within the test data set, the accuracy is 0.7093, kappa is 0.242 and sensitivity is 0.8020.

## Q5 2 limitations/considerations of the model generated by this analysis.
#### One limitation or consideration can be that the model generated by this analysis may not be applicable to other populations or future time points, as the original NHANES data is cross-sectional. The variables included in the model also might not be as important predictors as others. Secondly, any innate biases in the data can affect the accuracy of the model and require further studies to validate it. 
